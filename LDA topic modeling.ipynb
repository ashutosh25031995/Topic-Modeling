{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796f64bf-5239-4ea2-ac88-9737171711d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-21e541d77834>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# For visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text preprocessing libraries\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Downloading necessary NLTK datasets\n",
    "nltk.download('stopwords')  # Stopwords for removing common words\n",
    "nltk.download('omw-1.4')    # Open Multilingual Wordnet for lemmatization\n",
    "nltk.download('wordnet')     # WordNet lemmatizer\n",
    "nltk.download('punkt')       # Tokenizer for splitting sentences into words\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Import stopwords from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Vectorization libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# LDA (Latent Dirichlet Allocation) model\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0789c001-ee2e-48ae-89ec-761b3f2ef1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-21e541d77834>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# For visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text preprocessing libraries\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Downloading necessary NLTK datasets\n",
    "nltk.download('stopwords')  # Stopwords for removing common words\n",
    "nltk.download('omw-1.4')    # Open Multilingual Wordnet for lemmatization\n",
    "nltk.download('wordnet')     # WordNet lemmatizer\n",
    "nltk.download('punkt')       # Tokenizer for splitting sentences into words\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Import stopwords from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Vectorization libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# LDA (Latent Dirichlet Allocation) model\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f6ef6b7-b831-470a-8e77-067a053e26d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1f35821b3e90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# For visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m  \u001b[1;31m# Correct import for sklearn integration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text preprocessing libraries\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Downloading necessary NLTK datasets\n",
    "nltk.download('stopwords')  # Stopwords for removing common words\n",
    "nltk.download('omw-1.4')    # Open Multilingual Wordnet for lemmatization\n",
    "nltk.download('wordnet')     # WordNet lemmatizer\n",
    "nltk.download('punkt')       # Tokenizer for splitting sentences into words\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Import stopwords from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Vectorization libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# LDA (Latent Dirichlet Allocation) model\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn  # Correct import for sklearn integration\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbccb0ff-24ba-44aa-b388-458b7d99060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully from: C:/Users/lenovo/Desktop/Bert LLM 2/scopus.csv\n",
      "                                             Authors  \\\n",
      "0                        Steni Mol T.S.; Sreeja P.S.   \n",
      "1                          Subashini K.; Narmatha V.   \n",
      "2                           Guo Z.; Li J.; Ramesh R.   \n",
      "3                           Gotmare P.S.; Potey M.M.   \n",
      "4  Waisberg E.; Ong J.; Masalkhi M.; Zaman N.; Sa...   \n",
      "\n",
      "                                   Author full names  \\\n",
      "0  Steni Mol, T.S. (57971790100); Sreeja, P.S. (5...   \n",
      "1  Subashini, K. (55578386200); Narmatha, V. (572...   \n",
      "2  Guo, Zhiling (49861197800); Li, Jin (571961569...   \n",
      "3  Gotmare, Pradnya S. (57205389151); Potey, Mani...   \n",
      "4  Waisberg, Ethan (57225197689); Ong, Joshua (57...   \n",
      "\n",
      "                                        Author(s) ID  \\\n",
      "0                           57971790100; 56380319100   \n",
      "1                           55578386200; 57211296791   \n",
      "2               49861197800; 57196156947; 7201897779   \n",
      "3                           57205389151; 57189040816   \n",
      "4  57225197689; 57214792343; 57926028400; 5721219...   \n",
      "\n",
      "                                               Title  Year  \\\n",
      "0  Adam Adadelta Optimization based bidirectional...  2023   \n",
      "1  MANTA RAY FORAGING OPTIMIZATION ALGORITHM WITH...  2023   \n",
      "2  Green Data Analytics of Supercomputing from Ma...  2023   \n",
      "3  Combined Approach for Answer Identification wi...  2023   \n",
      "4  GPT-4 and medical image analysis: strengths, w...  2023   \n",
      "\n",
      "                                        Source title  \\\n",
      "0                        Multiagent and Grid Systems   \n",
      "1  Journal of Theoretical and Applied Information...   \n",
      "2                       Information Systems Research   \n",
      "3                  Revue d'Intelligence Artificielle   \n",
      "4         Journal of Medical Artificial Intelligence   \n",
      "\n",
      "                                                Link  \\\n",
      "0  https://scopus.duelibrary.in/inward/record.uri...   \n",
      "1  https://scopus.duelibrary.in/inward/record.uri...   \n",
      "2  https://scopus.duelibrary.in/inward/record.uri...   \n",
      "3  https://scopus.duelibrary.in/inward/record.uri...   \n",
      "4  https://scopus.duelibrary.in/inward/record.uri...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Social platform have disseminated the news in ...   \n",
      "1  In current scenario, phishing attacks are vita...   \n",
      "2  Energy costs represent a significant share of ...   \n",
      "3  In the realm of natural language understanding...   \n",
      "4  ChatGPT (Generative Pre-trained Transformer) i...   \n",
      "\n",
      "                                     Author Keywords  \\\n",
      "0  adam optimizer; Fake news detection; kernel li...   \n",
      "1  Cybersecurity; Deep Learning; Manta Ray Foragi...   \n",
      "2  autoregressive model; data analytics; data cen...   \n",
      "3  annotations; comprehension; embedding; semanti...   \n",
      "4  Artificial intelligence (AI); Generative Pre-t...   \n",
      "\n",
      "                                      Index Keywords  \n",
      "0  Discriminant analysis; Fake detection; Long sh...  \n",
      "1                                                NaN  \n",
      "2  Commerce; Data Analytics; Energy efficiency; G...  \n",
      "3                                                NaN  \n",
      "4                                                NaN  \n",
      "\n",
      "Columns available in the dataset:\n",
      " Index(['Authors', 'Author full names', 'Author(s) ID', 'Title', 'Year',\n",
      "       'Source title', 'Link', 'Abstract', 'Author Keywords',\n",
      "       'Index Keywords'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Import necessary module for file dialog\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "import pandas as pd\n",
    "\n",
    "# Open a file dialog to select the CSV file\n",
    "Tk().withdraw()  # Prevents the Tkinter window from showing up\n",
    "file_path = askopenfilename(title=\"Select the CSV file\", filetypes=[(\"CSV files\", \"*.csv\")])\n",
    "\n",
    "# Check if a file was selected\n",
    "if file_path:\n",
    "    try:\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Display the first few rows of the dataset to confirm it's loaded correctly\n",
    "        print(f\"File loaded successfully from: {file_path}\")\n",
    "        print(df.head())  # Display the first few rows\n",
    "        print(\"\\nColumns available in the dataset:\\n\", df.columns)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"The file at {file_path} is empty. Please choose a valid CSV file.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"There was a problem parsing the file at {file_path}. Ensure it's a valid CSV.\")\n",
    "    except Exception as e:\n",
    "        # Print any other error that occurs during file loading\n",
    "        print(f\"An unexpected error occurred while loading the file: {str(e)}\")\n",
    "else:\n",
    "    print(\"No file selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e645a6bb-e6f8-43e6-aa35-65f6d8e0f144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing data summary:\n",
      " Authors                 7\n",
      "Author full names       7\n",
      "Author(s) ID            7\n",
      "Title                   0\n",
      "Year                    0\n",
      "Source title            8\n",
      "Link                    0\n",
      "Abstract                0\n",
      "Author Keywords      1706\n",
      "Index Keywords       3615\n",
      "dtype: int64\n",
      "\n",
      "Rows with missing data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\anaconda3\\envs\\nlpcourse\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors</th>\n",
       "      <th>Author full names</th>\n",
       "      <th>Author(s) ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Source title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Author Keywords</th>\n",
       "      <th>Index Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subashini K.; Narmatha V.</td>\n",
       "      <td>Subashini, K. (55578386200); Narmatha, V. (572...</td>\n",
       "      <td>55578386200; 57211296791</td>\n",
       "      <td>MANTA RAY FORAGING OPTIMIZATION ALGORITHM WITH...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Journal of Theoretical and Applied Information...</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>In current scenario, phishing attacks are vita...</td>\n",
       "      <td>Cybersecurity; Deep Learning; Manta Ray Foragi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gotmare P.S.; Potey M.M.</td>\n",
       "      <td>Gotmare, Pradnya S. (57205389151); Potey, Mani...</td>\n",
       "      <td>57205389151; 57189040816</td>\n",
       "      <td>Combined Approach for Answer Identification wi...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Revue d'Intelligence Artificielle</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>In the realm of natural language understanding...</td>\n",
       "      <td>annotations; comprehension; embedding; semanti...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Waisberg E.; Ong J.; Masalkhi M.; Zaman N.; Sa...</td>\n",
       "      <td>Waisberg, Ethan (57225197689); Ong, Joshua (57...</td>\n",
       "      <td>57225197689; 57214792343; 57926028400; 5721219...</td>\n",
       "      <td>GPT-4 and medical image analysis: strengths, w...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Journal of Medical Artificial Intelligence</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>ChatGPT (Generative Pre-trained Transformer) i...</td>\n",
       "      <td>Artificial intelligence (AI); Generative Pre-t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pimentel A.; Guimarães H.R.; Avila A.; Falk T.H.</td>\n",
       "      <td>Pimentel, Arthur (57984648600); Guimarães, Hei...</td>\n",
       "      <td>57984648600; 57984466900; 56414301600; 7004897891</td>\n",
       "      <td>Environment-Aware Knowledge Distillation for I...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Applied Sciences (Switzerland)</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>Recent advances in self-supervised learning ha...</td>\n",
       "      <td>automatic speech recognition; context awarenes...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yun H.; Yi E.; Song S.</td>\n",
       "      <td>Yun, Hongoak (56146332000); Yi, Eunkyung (5721...</td>\n",
       "      <td>56146332000; 57210172220; 56652855600</td>\n",
       "      <td>Exploring AI-Generated English Relative Clause...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Journal of Cognitive Science</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>Human behavioral studies have consistently ind...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12552</th>\n",
       "      <td>Turtle D.P.; Phillipson P.H.</td>\n",
       "      <td>Turtle, D.P. (57199627939); Phillipson, P.H. (...</td>\n",
       "      <td>57199627939; 6701807880</td>\n",
       "      <td>Simultaneous identification and control</td>\n",
       "      <td>1971</td>\n",
       "      <td>Automatica</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>The problem of identification of a process und...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SYSTEM IDENTIFICATION; AUTOMATIC CONTROL--Anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12553</th>\n",
       "      <td>MEHAUTE LE B.</td>\n",
       "      <td>MEHAUTE LE, B. (6504468416)</td>\n",
       "      <td>6504468416</td>\n",
       "      <td>SPECIAL CONSIDERATION ON THE DESIGN OF AN LNG ...</td>\n",
       "      <td>1974</td>\n",
       "      <td>PROC. 14TH. ASCE COASTAL ENGNG. CONF. (COPENHA...</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>ONE OF THE PRIMARY CONSIDERATIONS IN THE DESIG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HARBOURS; L.N.G; TANKERS (SHIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12554</th>\n",
       "      <td>Akaike H.</td>\n",
       "      <td>Akaike, Hirotugu (7003985470)</td>\n",
       "      <td>7003985470</td>\n",
       "      <td>Canonical correlation analysis of time series ...</td>\n",
       "      <td>1976</td>\n",
       "      <td>Mathematics in Science and Engineering</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>This chapter starts with a brief introductory ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12555</th>\n",
       "      <td>Tong H.</td>\n",
       "      <td>Tong, Howell (7201359749)</td>\n",
       "      <td>7201359749</td>\n",
       "      <td>More on Autoregressive Model Fitting with Nois...</td>\n",
       "      <td>1977</td>\n",
       "      <td>IEEE Transactions on Information Theory</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>Tong has proposed an objective method, based o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INFORMATION THEORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12556</th>\n",
       "      <td>McClave J.</td>\n",
       "      <td>McClave, James (6507362916)</td>\n",
       "      <td>6507362916</td>\n",
       "      <td>Subset autoregression</td>\n",
       "      <td>1975</td>\n",
       "      <td>Technometrics</td>\n",
       "      <td>https://scopus.duelibrary.in/inward/record.uri...</td>\n",
       "      <td>The use of the autoregressive model for descri...</td>\n",
       "      <td>Autoregression; Final prediction error (fpe); ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5022 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Authors  \\\n",
       "1                              Subashini K.; Narmatha V.   \n",
       "3                               Gotmare P.S.; Potey M.M.   \n",
       "4      Waisberg E.; Ong J.; Masalkhi M.; Zaman N.; Sa...   \n",
       "6       Pimentel A.; Guimarães H.R.; Avila A.; Falk T.H.   \n",
       "8                                 Yun H.; Yi E.; Song S.   \n",
       "...                                                  ...   \n",
       "12552                       Turtle D.P.; Phillipson P.H.   \n",
       "12553                                      MEHAUTE LE B.   \n",
       "12554                                          Akaike H.   \n",
       "12555                                            Tong H.   \n",
       "12556                                         McClave J.   \n",
       "\n",
       "                                       Author full names  \\\n",
       "1      Subashini, K. (55578386200); Narmatha, V. (572...   \n",
       "3      Gotmare, Pradnya S. (57205389151); Potey, Mani...   \n",
       "4      Waisberg, Ethan (57225197689); Ong, Joshua (57...   \n",
       "6      Pimentel, Arthur (57984648600); Guimarães, Hei...   \n",
       "8      Yun, Hongoak (56146332000); Yi, Eunkyung (5721...   \n",
       "...                                                  ...   \n",
       "12552  Turtle, D.P. (57199627939); Phillipson, P.H. (...   \n",
       "12553                        MEHAUTE LE, B. (6504468416)   \n",
       "12554                      Akaike, Hirotugu (7003985470)   \n",
       "12555                          Tong, Howell (7201359749)   \n",
       "12556                        McClave, James (6507362916)   \n",
       "\n",
       "                                            Author(s) ID  \\\n",
       "1                               55578386200; 57211296791   \n",
       "3                               57205389151; 57189040816   \n",
       "4      57225197689; 57214792343; 57926028400; 5721219...   \n",
       "6      57984648600; 57984466900; 56414301600; 7004897891   \n",
       "8                  56146332000; 57210172220; 56652855600   \n",
       "...                                                  ...   \n",
       "12552                            57199627939; 6701807880   \n",
       "12553                                         6504468416   \n",
       "12554                                         7003985470   \n",
       "12555                                         7201359749   \n",
       "12556                                         6507362916   \n",
       "\n",
       "                                                   Title  Year  \\\n",
       "1      MANTA RAY FORAGING OPTIMIZATION ALGORITHM WITH...  2023   \n",
       "3      Combined Approach for Answer Identification wi...  2023   \n",
       "4      GPT-4 and medical image analysis: strengths, w...  2023   \n",
       "6      Environment-Aware Knowledge Distillation for I...  2023   \n",
       "8      Exploring AI-Generated English Relative Clause...  2023   \n",
       "...                                                  ...   ...   \n",
       "12552            Simultaneous identification and control  1971   \n",
       "12553  SPECIAL CONSIDERATION ON THE DESIGN OF AN LNG ...  1974   \n",
       "12554  Canonical correlation analysis of time series ...  1976   \n",
       "12555  More on Autoregressive Model Fitting with Nois...  1977   \n",
       "12556                              Subset autoregression  1975   \n",
       "\n",
       "                                            Source title  \\\n",
       "1      Journal of Theoretical and Applied Information...   \n",
       "3                      Revue d'Intelligence Artificielle   \n",
       "4             Journal of Medical Artificial Intelligence   \n",
       "6                         Applied Sciences (Switzerland)   \n",
       "8                           Journal of Cognitive Science   \n",
       "...                                                  ...   \n",
       "12552                                         Automatica   \n",
       "12553  PROC. 14TH. ASCE COASTAL ENGNG. CONF. (COPENHA...   \n",
       "12554             Mathematics in Science and Engineering   \n",
       "12555            IEEE Transactions on Information Theory   \n",
       "12556                                      Technometrics   \n",
       "\n",
       "                                                    Link  \\\n",
       "1      https://scopus.duelibrary.in/inward/record.uri...   \n",
       "3      https://scopus.duelibrary.in/inward/record.uri...   \n",
       "4      https://scopus.duelibrary.in/inward/record.uri...   \n",
       "6      https://scopus.duelibrary.in/inward/record.uri...   \n",
       "8      https://scopus.duelibrary.in/inward/record.uri...   \n",
       "...                                                  ...   \n",
       "12552  https://scopus.duelibrary.in/inward/record.uri...   \n",
       "12553  https://scopus.duelibrary.in/inward/record.uri...   \n",
       "12554  https://scopus.duelibrary.in/inward/record.uri...   \n",
       "12555  https://scopus.duelibrary.in/inward/record.uri...   \n",
       "12556  https://scopus.duelibrary.in/inward/record.uri...   \n",
       "\n",
       "                                                Abstract  \\\n",
       "1      In current scenario, phishing attacks are vita...   \n",
       "3      In the realm of natural language understanding...   \n",
       "4      ChatGPT (Generative Pre-trained Transformer) i...   \n",
       "6      Recent advances in self-supervised learning ha...   \n",
       "8      Human behavioral studies have consistently ind...   \n",
       "...                                                  ...   \n",
       "12552  The problem of identification of a process und...   \n",
       "12553  ONE OF THE PRIMARY CONSIDERATIONS IN THE DESIG...   \n",
       "12554  This chapter starts with a brief introductory ...   \n",
       "12555  Tong has proposed an objective method, based o...   \n",
       "12556  The use of the autoregressive model for descri...   \n",
       "\n",
       "                                         Author Keywords  \\\n",
       "1      Cybersecurity; Deep Learning; Manta Ray Foragi...   \n",
       "3      annotations; comprehension; embedding; semanti...   \n",
       "4      Artificial intelligence (AI); Generative Pre-t...   \n",
       "6      automatic speech recognition; context awarenes...   \n",
       "8                                                    NaN   \n",
       "...                                                  ...   \n",
       "12552                                                NaN   \n",
       "12553                                                NaN   \n",
       "12554                                                NaN   \n",
       "12555                                                NaN   \n",
       "12556  Autoregression; Final prediction error (fpe); ...   \n",
       "\n",
       "                                          Index Keywords  \n",
       "1                                                    NaN  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "6                                                    NaN  \n",
       "8                                                    NaN  \n",
       "...                                                  ...  \n",
       "12552  SYSTEM IDENTIFICATION; AUTOMATIC CONTROL--Anal...  \n",
       "12553                    HARBOURS; L.N.G; TANKERS (SHIPS  \n",
       "12554                                                NaN  \n",
       "12555                                 INFORMATION THEORY  \n",
       "12556                                                NaN  \n",
       "\n",
       "[5022 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample cleaned documents (first 5):\n",
      "\n",
      "Document 1:\n",
      "Adam Adadelta Optimization based bidirectional encoder representations from transformers model for fake news detection on social media. social platform disseminated news rapid speed considered important news resource many people worldwide easy access le cost benefit compared traditional news organization fake news news deliberately written bad writer manipulates original content rapid dissemination fake news may mislead people society result critical investigate veracity data leaked via social medium platform even reliability information reported via platform still doubtful remains significant obstacle result proposes promising technique identifying fake information social medium called adam adadelta optimization based deep long shortterm memory deep lstm tokenization operation case carried bidirectional encoder representation transformer bert approach measurement feature reduced assistance kernel linear discriminant analysis lda singular value decomposition svd topn attribute chosen employing renyi joint entropy furthermore lstm applied identify false information social medium adam adadelta optimization comprises combo adam optimization adadelta optimization deep lstm based adam adadelta optimization achieved maximum accuracy sensitivity specificity io press right reserved\n",
      "\n",
      "Document 2:\n",
      "MANTA RAY FORAGING OPTIMIZATION ALGORITHM WITH DEEP LEARNING ASSISTED AUTOMATED PHISHING URL DETECTION MODEL. current scenario phishing attack vital threat cyberspace security phishing one common type scam attract individual access mischievous url uniform resource locator well personal data like id password others many intelligent attack launched cheat user retrieving trustworthy website online platform order get data phishing url classification one crucial cybersecurity task intended classify moderate malevolent web address considered cheat consumer revealing sensitive data numerous researcher cyberspace interested generating intelligent technique well offering security service phishing website grows clever malicious daily therefore introduces manta ray foraging optimization deep learningbased phishing website detection mrfodlpwd technique major intention mrfodlpwd technique recognize classify presence legitimate phishing url presented mrfodlpwd technique several stage preprocessing transfer data useful setup bert applied feature extraction moreover deep belief network dbn used automated phishing url detection furthermore mrfo algorithm selects hyperparameter value dbn extensive comparison stated mrfodlpwd technique accomplishes enhanced phishing url detection result model little lion scientific\n",
      "\n",
      "Document 3:\n",
      "Combined Approach for Answer Identification with Small Sized Reading Comprehension Datasets. realm natural language understanding machine reading comprehension emerged significant area interest requiring machine extract pertinent information textual data understand proposes novel answer identification multiplechoice question answering setup utilizing science textbook narrative text data proposed methodology integrates lexical semantic feature word sentencelevel equivalence initially strategy exploit lexical feature particularly word overlap critical answer identification extract feature noun phrase verb phrase preposition accounting grammatical relationship feature enhanced assessing semantic similarity via transformer subsequently answer identification executed mapping answer option sentence paragraph sentence onetoone basis accuracy correct answer identification evaluated using featurebased approach bertbased approach result indicated accuracy science narrative datasets respectively employing combined approach performance evaluation proposed undertaken finetuned pretrained language evaluation analysis revealed certain challenge proposed methodology outlining avenue future research copyright iieta\n",
      "\n",
      "Document 4:\n",
      "GPT-4 and medical image analysis: strengths, weaknesses and future directions. chatgpt generative pretrained transformer artificial intelligence language developed openai newest version chatgpt released march reported broader knowledge base well improved problemsolving ability also reported le easy fool capable processing time word usage chatgpt continue grow new application langue learning continue found due black box nature model interpretation output must made caution ensure error made particularly healthcare delivery medicine policy procedure frequently revised algorithm comment may outofdate incorrect new feature introduced important feature may new ability analyze image could potentially help doctor diagnose treat patient quickly accurately especially area access medical professional may limited examine gpts image diagnostic ability provided variety common medical imaging modality chest xrays magnetic resonance image mri optical coherence tomography oct image although significant advancement research still required future automated medical image analysis highly promising journal medical artificial intelligence right reserved\n",
      "\n",
      "Document 5:\n",
      "AlgBERT: Automatic Construction of Annotated Corpus for Sentiment Analysis in Algerian Dialect. nowadays sentiment analysis one crucial research field natural language processing nlp widely applied variety application marketing politics however arabic language still lack sufficient language resource enable task opinion emotion analysis comparing language english additionally manual annotation requires lot effort time article address problem propose novel automated annotation platform sentiment analysis called algbert providing annotated corpus using deep learning includes many automatic natural language processing algorithm basis text classification opinion analysis suggest using bert abbreviation bidirectional encoder representation transformer one effective technology term result different world language used around comment collected social networking twitter youtube written arabic algerian dialect algbert system obtained excellent result accuracy considered one best result opinion analysis algerian dialect copyright held ownerauthors\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Text</th>\n",
       "      <th>Year</th>\n",
       "      <th>Author Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adam Adadelta Optimization based bidirectional...</td>\n",
       "      <td>social platform disseminated news rapid speed ...</td>\n",
       "      <td>Adam Adadelta Optimization based bidirectional...</td>\n",
       "      <td>2023</td>\n",
       "      <td>adam optimizer; Fake news detection; kernel li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MANTA RAY FORAGING OPTIMIZATION ALGORITHM WITH...</td>\n",
       "      <td>current scenario phishing attack vital threat ...</td>\n",
       "      <td>MANTA RAY FORAGING OPTIMIZATION ALGORITHM WITH...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Cybersecurity; Deep Learning; Manta Ray Foragi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combined Approach for Answer Identification wi...</td>\n",
       "      <td>realm natural language understanding machine r...</td>\n",
       "      <td>Combined Approach for Answer Identification wi...</td>\n",
       "      <td>2023</td>\n",
       "      <td>annotations; comprehension; embedding; semanti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPT-4 and medical image analysis: strengths, w...</td>\n",
       "      <td>chatgpt generative pretrained transformer arti...</td>\n",
       "      <td>GPT-4 and medical image analysis: strengths, w...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Artificial intelligence (AI); Generative Pre-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AlgBERT: Automatic Construction of Annotated C...</td>\n",
       "      <td>nowadays sentiment analysis one crucial resear...</td>\n",
       "      <td>AlgBERT: Automatic Construction of Annotated C...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Annotated corpus; BERT; deep learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Adam Adadelta Optimization based bidirectional...   \n",
       "1  MANTA RAY FORAGING OPTIMIZATION ALGORITHM WITH...   \n",
       "3  Combined Approach for Answer Identification wi...   \n",
       "4  GPT-4 and medical image analysis: strengths, w...   \n",
       "5  AlgBERT: Automatic Construction of Annotated C...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  social platform disseminated news rapid speed ...   \n",
       "1  current scenario phishing attack vital threat ...   \n",
       "3  realm natural language understanding machine r...   \n",
       "4  chatgpt generative pretrained transformer arti...   \n",
       "5  nowadays sentiment analysis one crucial resear...   \n",
       "\n",
       "                                                Text  Year  \\\n",
       "0  Adam Adadelta Optimization based bidirectional...  2023   \n",
       "1  MANTA RAY FORAGING OPTIMIZATION ALGORITHM WITH...  2023   \n",
       "3  Combined Approach for Answer Identification wi...  2023   \n",
       "4  GPT-4 and medical image analysis: strengths, w...  2023   \n",
       "5  AlgBERT: Automatic Construction of Annotated C...  2023   \n",
       "\n",
       "                                     Author Keywords  \n",
       "0  adam optimizer; Fake news detection; kernel li...  \n",
       "1  Cybersecurity; Deep Learning; Manta Ray Foragi...  \n",
       "3  annotations; comprehension; embedding; semanti...  \n",
       "4  Artificial intelligence (AI); Generative Pre-t...  \n",
       "5              Annotated corpus; BERT; deep learning  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "wn = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add custom stopwords specific to your context (LLM/AI)\n",
    "custom_stopwords = {\"gpt\", \"group\", \"model\", \"study\", \"level\", \"activity\", \"method\", \"technology\", \"patient\"}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "# Define a list of keywords relevant to LLMs and generative AI to filter relevant documents\n",
    "llm_keywords = [\"transformer\", \"bert\", \"gpt\", \"generative ai\", \"large language model\", \"neural network\", \n",
    "                \"deep learning\", \"transfer learning\", \"fine-tuning\", \"autoregressive\", \"pretrained\"]\n",
    "\n",
    "# Step 1: Check for missing data in the dataset\n",
    "missing_data = df.isnull().sum()\n",
    "print(\"\\nMissing data summary:\\n\", missing_data)\n",
    "\n",
    "# Display rows with missing values, if any\n",
    "if missing_data.any():\n",
    "    print(\"\\nRows with missing data:\")\n",
    "    display(df[df.isnull().any(axis=1)])\n",
    "else:\n",
    "    print(\"\\nNo missing data found in the dataset.\")\n",
    "\n",
    "# Step 2: Clean missing abstracts\n",
    "df_clean = df.dropna(subset=['Abstract'])\n",
    "\n",
    "# Step 3: Convert abstracts to lowercase (Vectorized operation)\n",
    "df_clean['Abstract'] = df_clean['Abstract'].str.lower()\n",
    "\n",
    "# Step 4: Filter documents based on LLM-related keywords\n",
    "# Use the vectorized `str.contains` method for faster filtering\n",
    "pattern = '|'.join(llm_keywords)\n",
    "df_clean = df_clean[df_clean['Abstract'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "# Step 5: Remove special characters using regex\n",
    "# More efficient vectorized string operation\n",
    "df_clean['Abstract'] = df_clean['Abstract'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "\n",
    "# Step 6: Lemmatize and remove stopwords (vectorized for better performance)\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
    "    lemmatized_tokens = [wn.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "df_clean['Abstract'] = df_clean['Abstract'].apply(preprocess_text)\n",
    "\n",
    "# Step 7: Combine 'Title' and 'Abstract' into a single text column\n",
    "df_clean['Text'] = df_clean['Title'].fillna('') + \". \" + df_clean['Abstract']\n",
    "\n",
    "# Step 8: Retain relevant columns\n",
    "df_clean['Year'] = df_clean['Year']  # Already exists, no need to reassign but kept for clarity\n",
    "df_clean['Author Keywords'] = df_clean['Author Keywords'].fillna('')  # Fill missing keywords with an empty string\n",
    "\n",
    "# Step 9: Clean the \"No abstract available\" text\n",
    "df_clean['Text'] = df_clean['Text'].str.replace('[no abstract available]', '', regex=False).str.strip()\n",
    "\n",
    "# Step 10: Display a few sample cleaned documents to verify\n",
    "print(\"\\nSample cleaned documents (first 5):\\n\")\n",
    "for i, doc in enumerate(df_clean['Text'].head(5), 1):\n",
    "    print(f\"Document {i}:\\n{doc}\\n\")\n",
    "\n",
    "# Step 11: Return the cleaned DataFrame with all relevant columns\n",
    "df_clean = df_clean[['Title', 'Abstract', 'Text', 'Year', 'Author Keywords']]\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332d1181-3173-4505-b137-705c3775356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #1:\n",
      "model forecasting autoregressive forecast price series time market prediction vector\n",
      "\n",
      "Topic #2:\n",
      "spatial data model autoregressive traffic bayesian analysis effect using variable\n",
      "\n",
      "Topic #3:\n",
      "language bert text model sentiment classification word using analysis task\n",
      "\n",
      "Topic #4:\n",
      "knowledge entity extraction information question based text bert graph relation\n",
      "\n",
      "Topic #5:\n",
      "user code recommendation generation topic software review based source approach\n",
      "\n",
      "Topic #6:\n",
      "learning network feature transformer task representation data information attention deep\n",
      "\n",
      "Topic #7:\n",
      "springer response design problem nature structure robot theory paper framework\n",
      "\n",
      "Topic #8:\n",
      "image training learning domain data segmentation object proposed approach method\n",
      "\n",
      "Topic #9:\n",
      "autoregressive model time series parameter process estimation data estimator distribution\n",
      "\n",
      "Topic #10:\n",
      "model language learning data research news using task large deep\n",
      "\n",
      "Topic #11:\n",
      "energy effect result change temperature time wind flow impact policy\n",
      "\n",
      "Topic #12:\n",
      "network prediction feature neural data using proposed based accuracy result\n",
      "\n",
      "Topic #13:\n",
      "signal autoregressive algorithm using control proposed based noise parameter estimation\n",
      "\n",
      "Topic #14:\n",
      "transformer power model current paper frequency simulation result analysis winding\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# Assuming df_clean['Text'] contains the cleaned data\n",
    "\n",
    "# Step 1: Fine-tuning vectorizer (CountVectorizer)\n",
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.95,  \n",
    "    min_df=5,  \n",
    "    stop_words='english',  \n",
    "    max_features=2000\n",
    ")\n",
    "\n",
    "# Create document-term matrix\n",
    "dtm = vectorizer.fit_transform(df_clean['Text'])\n",
    "\n",
    "# Step 2: Fine-tuned LDA model\n",
    "n_topics = 14  \n",
    "lda_model = LDA(\n",
    "    n_components=n_topics,  \n",
    "    learning_decay=0.7,  \n",
    "    max_iter=15,  \n",
    "    random_state=42,  \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the LDA model on the document-term matrix\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Step 3: Function to display top words per topic\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic #{topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "# Display the top 10 words for each of the 14 topics\n",
    "num_top_words = 10  \n",
    "feature_names = vectorizer.get_feature_names()  # Fixed method\n",
    "display_topics(lda_model, feature_names, num_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc9da58-9e3b-4137-90da-ac4ec00fbe95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-04d6a4303b3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Create document-term matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Step 2: Fine-tuned LDA model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for LDA and vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# Step 1: Fine-tuning vectorizer (CountVectorizer)\n",
    "# Fine-tuned parameters: adjusting max_df, min_df, max_features\n",
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.95,  # Ignore terms that appear in more than 95% of the documents\n",
    "    min_df=5,  # Include terms that appear in at least 5 documents\n",
    "    stop_words='english',  # Remove English stopwords\n",
    "    max_features=2000  # Limit vocabulary to the top 2000 words by frequency\n",
    ")\n",
    "\n",
    "# Create document-term matrix\n",
    "dtm = vectorizer.fit_transform(df_clean['Text'])\n",
    "\n",
    "# Step 2: Fine-tuned LDA model\n",
    "# Parameters: \n",
    "# - n_components = 14 (number of topics)\n",
    "# - learning_decay = 0.7 (controls learning rate, balancing stability vs convergence)\n",
    "# - max_iter = 15 (to allow better convergence)\n",
    "# - random_state = 42 (for reproducibility)\n",
    "\n",
    "n_topics = 14  # Number of topics\n",
    "lda_model = LDA(\n",
    "    n_components=n_topics,  # 14 topics\n",
    "    learning_decay=0.7,  # Tune learning decay to 0.7 for better stability\n",
    "    max_iter=15,  # Increase iterations to help convergence\n",
    "    random_state=42,  # Ensure reproducibility\n",
    "    n_jobs=-1  # Use all available CPU cores to speed up training\n",
    ")\n",
    "\n",
    "# Fit the LDA model on the document-term matrix\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Step 3: Function to display top words per topic\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic #{topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "# Display the top 10 words for each of the 14 topics\n",
    "num_top_words = 10  # Display top 10 words\n",
    "feature_names = vectorizer.get_feature_names()  # Fixed method\n",
    "display_topics(lda_model, feature_names, num_top_words)\n",
    "\n",
    "# Optional: Print out the LDA model parameters\n",
    "print(\"\\nLDA Model Parameters:\")\n",
    "print(f\"Number of Topics: {n_topics}\")\n",
    "print(f\"Max Features: {vectorizer.max_features}\")\n",
    "print(f\"Max DF: {vectorizer.max_df}\")\n",
    "print(f\"Min DF: {vectorizer.min_df}\")\n",
    "print(f\"Learning Decay: {lda_model.learning_decay}\")\n",
    "print(f\"Max Iterations: {lda_model.max_iter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678d3d88",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bab4e69a72cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Use libraries like `spacy` for this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlpcourse\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlpcourse\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Vectorizer settings: increase max_features, use bigrams, adjust min_df and max_df\n",
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.90,  # Ignore terms that appear in more than 90% of the documents\n",
    "    min_df=10,  # Include terms that appear in at least 10 documents\n",
    "    stop_words='english',  # Remove English stopwords\n",
    "    max_features=5000,  # Increase vocabulary size to 5000\n",
    "    ngram_range=(1, 2)  # Include both unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Adjusted LDA model parameters\n",
    "lda_model = LDA(\n",
    "    n_components=16,  # Increase topics to 16 to capture finer distinctions\n",
    "    learning_decay=0.5,  # Use a lower learning decay for faster convergence\n",
    "    max_iter=30,  # More iterations for better convergence\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Optional preprocessing: apply lemmatization before fitting the vectorizer\n",
    "# Use libraries like `spacy` for this\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "\n",
    "df_clean['Text'] = df_clean['Text'].apply(lemmatize)\n",
    "\n",
    "# Now, apply the vectorizer and fit the LDA model as before\n",
    "dtm = vectorizer.fit_transform(df_clean['Text'])\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Use pyLDAvis for topic visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, dtm, vectorizer)\n",
    "panel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a86b912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #1:\n",
      "autoregressive series time estimator time series test parameter process order estimation\n",
      "\n",
      "Topic #2:\n",
      "springer news nature springer nature fake medium exclusive licence author exclusive author exclusive licence\n",
      "\n",
      "Topic #3:\n",
      "transformer power wind frequency current voltage paper transient simulation circuit\n",
      "\n",
      "Topic #4:\n",
      "transformer feature propose network attention deep convolutional dataset image sequence\n",
      "\n",
      "Topic #5:\n",
      "user research provide generation language generate design large human paper\n",
      "\n",
      "Topic #6:\n",
      "language bert text dataset task transformer pretraine natural natural language representation\n",
      "\n",
      "Topic #7:\n",
      "classification sentiment feature analysis sentiment analysis text emotion accuracy base aspect\n",
      "\n",
      "Topic #8:\n",
      "signal autoregressive detection analysis frequency eeg spectral datum time detect\n",
      "\n",
      "Topic #9:\n",
      "network neural neural network machine learn deep deep neural learning recurrent deep neural network\n",
      "\n",
      "Topic #10:\n",
      "image propose algorithm estimation base filter noise signal autoregressive channel\n",
      "\n",
      "Topic #11:\n",
      "covid switzerland licensee author licensee mdpi author licensee mdpi licensee mdpi basel mdpi basel licensee mdpi basel\n",
      "\n",
      "Topic #12:\n",
      "price effect energy rate vector economic autoregressive vector autoregressive impact policy\n",
      "\n",
      "Topic #13:\n",
      "time prediction series datum forecast time series forecasting autoregressive predict traffic\n",
      "\n",
      "Topic #14:\n",
      "disease clinical medical risk stock causality datum volatility financial interaction\n",
      "\n",
      "Topic #15:\n",
      "bayesian markov prior wiley propose autoregressive john son wiley son john wiley\n",
      "\n",
      "Topic #16:\n",
      "learn learning datum task dataset training selfsupervise image label propose\n",
      "\n",
      "Topic #17:\n",
      "information knowledge graph semantic entity propose base text extraction bert\n",
      "\n",
      "Topic #18:\n",
      "nonlinear algorithm parameter autoregressive propose identification fault base dynamic input\n",
      "\n",
      "Topic #19:\n",
      "control surface present temperature wave material structure theory magnetic element\n",
      "\n",
      "Topic #20:\n",
      "spatial datum autoregressive estimate error matrix correlation effect linear analysis\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6d9dc3b3e329>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# Step 4: Use pyLDAvis for topic visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import spacy\n",
    "\n",
    "# Optional preprocessing: apply lemmatization and custom stop words\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a custom stopword list (can be extended based on domain-specific needs)\n",
    "custom_stopwords = ['model', 'data', 'result', 'approach', 'based', 'using']\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.lemma_ not in custom_stopwords])\n",
    "\n",
    "# Apply lemmatization\n",
    "df_clean['Text'] = df_clean['Text'].apply(lemmatize)\n",
    "\n",
    "# Vectorizer settings: increase max_features, use bigrams/trigrams, and adjust min_df/max_df\n",
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.85,  # Ignore terms that appear in more than 85% of the documents\n",
    "    min_df=10,  # Include terms that appear in at least 10 documents\n",
    "    stop_words='english',  # Remove English stopwords\n",
    "    max_features=5000,  # Increase vocabulary size to 5000\n",
    "    ngram_range=(1, 3)  # Include both unigrams, bigrams, and trigrams\n",
    ")\n",
    "\n",
    "# Create document-term matrix\n",
    "dtm = vectorizer.fit_transform(df_clean['Text'])\n",
    "\n",
    "# Adjusted LDA model parameters\n",
    "lda_model = LDA(\n",
    "    n_components=20,  # Try increasing topics to 20 for finer distinctions\n",
    "    learning_decay=0.6,  # Adjust learning decay for better convergence\n",
    "    max_iter=40,  # Increase iterations for better convergence\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the LDA model\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Step 3: Function to display top words per topic\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic #{topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "# Display the top 10 words for each topic\n",
    "num_top_words = 10  \n",
    "feature_names = vectorizer.get_feature_names()  \n",
    "display_topics(lda_model, feature_names, num_top_words)\n",
    "\n",
    "# Step 4: Use pyLDAvis for topic visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, dtm, vectorizer)\n",
    "panel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b63b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
